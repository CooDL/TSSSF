[OS]
save_dir = saves/ptb
word_file = %(save_dir)s/words.txt
tag_file = %(save_dir)s/tags.txt
rel_file = %(save_dir)s/rels.txt
embed_dir = data/
embed_file = data/glove.6B.100d.txt
data_dir = data/
train_file = %(data_dir)s/train.pos.replace
valid_file = %(data_dir)s/train.pos.replace
test_file = %(data_dir)s/test.pos.replace
embed_file_stack = data/glove.twitter.200d.txt

[Dataset]
cased = False
min_occur_count = 2
n_bkts = 30
n_valid_bkts = 1
lines_per_buffer = 0
load_emb = True
use_unk = False
stack = True
min_occur_count_stack = 2

[Layers]
n_recur = 4
n_mlp = 1
recur_cell = LSTMCell
recur_bidir = True
forget_bias = 0
stack_n_recur = 1
stack_n_mlp = 1

[Sizes]
embed_size = 100
recur_size = 150
mlp_size = 100
stack_embed_size = 200
stack_recur_size = 150
stack_mlp_size = 100

[Functions]
recur_func = tanh
mlp_func = elu

[Regularization]
l2_reg = 2e-6
recur_reg = 0
covar_reg = 0
ortho_reg = 0

[Dropout]
drop_gradually = False
word_keep_prob = 1
tag_keep_prob = .85
rel_keep_prob = 1
recur_keep_prob = .6
ff_keep_prob = .6
mlp_keep_prob = .67

[Learning rate]
learning_rate = 2e-3
decay = .75
decay_steps = 2000
clip = 15

[Radam]
mu = .9
nu = .9
gamma = 0
chi = 0
epsilon = 1e-12

[Training]
pretrain_iters = 1000
train_iters = 60000
train_batch_size = 50
test_batch_size = 0
validate_every = 50
print_every = 50
save_every = 50
per_process_gpu_memory_fraction = .65

